{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load a Small Dataset\n",
        "WMT14 English-German dataset"
      ],
      "metadata": {
        "id": "AUoDHsDge010"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\", split='train[:1000]')\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "train_dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_data = train_dataset['train']\n",
        "valid_data = train_dataset['test']"
      ],
      "metadata": {
        "id": "sN5FX-8bV2wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    inputs = [example['en'] for example in examples['translation']]\n",
        "    targets = [example['de'] for example in examples['translation']]\n",
        "    # Enable padding and truncation\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Apply the tokenizer to the datasets\n",
        "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_valid_data = valid_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2615bd8f8819456d9b032e628a1bdb3c",
            "41dfae33bc86412cb953c7548d26e5e2",
            "137fdb1554234259b09abcd22e2753f8",
            "255d1a44587a4a7fb2acca1c6831783e",
            "8aa1446e85d74bab9066850fe3e218f5",
            "5aab06c10e954d66b3c5e25275fb5ef6",
            "7efd2f734f9e44bbb7cd8b6e1bfffba6",
            "f8dc9d73fda64009a88c15c1fdea84a9",
            "e3de03f3734347b699e1ba59cb53e5e2",
            "621125b60ada451b9729fa761f89c068",
            "6ec19cec49424b7b9af858584061fcc6",
            "9926b251fe374c0daa7818ba877413b7",
            "f1ba4b68b7fd42e9ac940d2a1592ea6c",
            "a88e1243daf24a76a7f9b58d82685469",
            "8bad0d23fc0c48bebe6274b1423b3ac3",
            "56d0d0eb80084957a2fac1f6d6d63f3d",
            "db8b3765984448bf94f3dc6e0a32b4d2",
            "e8c7562f10f6457f9bc1cf6b677718b0",
            "810879ac49964de7985f6e08672f72d2",
            "410926ed1ff943dfbd9e9506feb4e2a6",
            "144368cf98de4d23a2458859b4160a75",
            "01fef5bd6b574e8cad893fd89c3f88ef"
          ]
        },
        "id": "KcLauV9WWGqe",
        "outputId": "7a50f93c-ecb1-41f9-9337-a35006c42996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2615bd8f8819456d9b032e628a1bdb3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9926b251fe374c0daa7818ba877413b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Config\n",
        "\n",
        "# Load the model configuration and the model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "MURymTEcWI78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_data,\n",
        "    eval_dataset=tokenized_valid_data,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "I0VJFevvXWF7",
        "outputId": "e55693ec-42e9-4cbe-a7a7-df434660319e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [675/675 04:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.174107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.149673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.638800</td>\n",
              "      <td>0.144700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=675, training_loss=1.2672367519802517, metrics={'train_runtime': 272.1203, 'train_samples_per_second': 9.922, 'train_steps_per_second': 2.481, 'total_flos': 365422863974400.0, 'train_loss': 1.2672367519802517, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Evaluation"
      ],
      "metadata": {
        "id": "nqo7wQ7gfFkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Evaluation Results: {eval_results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "aVmxJZE8fIEG",
        "outputId": "22811acf-1f41-45df-9ce9-518a47aa8aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.14470025897026062, 'eval_runtime': 2.96, 'eval_samples_per_second': 33.784, 'eval_steps_per_second': 8.446, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Translations"
      ],
      "metadata": {
        "id": "eM6C_fe0fLLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "sample_text = \"The house is wonderful.\"\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generate translation\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
        "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Translation: {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPWgaUq4fMO6",
        "outputId": "aee9c2ec-103f-414c-d033-71edf2b71a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: Das Haus ist wunderbar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXJMYFtljT1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}