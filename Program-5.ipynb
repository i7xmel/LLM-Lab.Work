{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHx+ZcjrVh9DlHFVbHX106"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "48249311719b4c42924413c9a9951769",
            "5b6e4649d67c4578a66bf1eacf6d7416",
            "a6bb3546d8c44b489093af5a7150de5d",
            "81660e111a314ec1879fdfc202ca4d03",
            "f5ffc151351b480abc24c8253bac4b65",
            "2b8f1ce776384aeeaf3e88e6e85e5958",
            "f23725883e174e318c2523386e825568",
            "54f8db5a93f141a3965bb5bf3862500c",
            "c573b6d4e74346bfa17fce2319e1f180",
            "47a5bf23427e4e78bb2588e15f4ae3f3",
            "0f1869929c0e4342aad7ed59ab6399af",
            "91a589c52118400aa99eb5163ea7a940",
            "e4563570da5840488accaa3474d3d5a9",
            "9b0c98dc7957444b8a60149f68b9bea6",
            "19fb481467cb45ccaa0aaaeb5c49431a",
            "30b8f58d16d346db8071d509ed86dead",
            "d9df07f893f646f99c6c3304097d2afe",
            "d9591ba285024a2a916f6f0ab724fe41",
            "12d2ffb1217a473f9c9da949644bd500",
            "28f5a407f0ab46179f1c08c4f8d5ff91",
            "a952b425a1454904a401bcca1f623962",
            "3f2b9fe3a33e4b89a438cee17f90888f",
            "c2426eee4fe7464cac7b6efdf2fc956c",
            "3f52244e04624113985437ea8645abca",
            "3d3813dee84842d28ce235ee9302d498",
            "65405d93f7314708b3474fd72846e310",
            "4a020c7602854358a53bb0641983b820",
            "5c940efd5f9d4d2086e3b84dbd5af7f4",
            "ea0efd556e4b480faa46788febaa2ece",
            "dc9e1943d46f4d599a7692bd45bf5301",
            "aa42904b8f3042e4979ca1db0cc863d6",
            "5dba6bf7158749989b5ffa6205398225",
            "933e7d0b3ed545b3afa223dbc0cd6f63",
            "0346d1a7710b45bebd120b2ec6c15413",
            "903bafd3f2ee4059b6f69c39b3b2ca42",
            "08356108e21e45b485f3fd56a3ec0e3c",
            "1c3a191c93e646b3979c7e187dbd66a3",
            "ba64da084c3841a98ba310075aa1c9a6",
            "029ae7f860114c64b175abe7d27d3d3f",
            "12f3f05f4f52496592157c57431f4639",
            "4a308ca40adb4cf98cbf3838b7e008b2",
            "05a6b8a57c114f7e9b5620adf292fe79",
            "4f4604fa3a1244a3ad7f5c67b7fa123a",
            "050e43efc0d44eed8d64cc8b179d39df",
            "ec719bf4e5274f5dbdfb77c8aae06170",
            "c5eead1f90a246e78246fdb506d4243c",
            "d6bc3f2b4fbf4dc59f86a79b108e680a",
            "a5411722c303439eaa9fe832f3e9d4e3",
            "9162285b7b634e5bad57f74819dd9886",
            "027f4f80153d4b23a51db68f7b872a1c",
            "a57847aed2644941b7bc6adbd218ce7e",
            "eb545021523b4c98a11e12dc02bb491f",
            "e84ee6aea1624ae39336c22bad610dd4",
            "a1794578747b46e2afc4fd4d1d4bb2e6",
            "2de9ae2ec2194da79bb96f1e894b2305",
            "fa0d3cc92e4b40b48d13a34eaaed7bad",
            "a60502bbe3204bbc91ef3e070d1da01a",
            "3bc09f80be2d4b8dabdcac9a7d800a86",
            "c6108a7bed744074ab8bce1bab6d3683",
            "f6796c6da9cd49a286ec6f7a026f19a6",
            "4f48db63a2114470a71ac9260cca7336",
            "2203c5976a004ebb982ddf390527a0c4",
            "6efd585af61e494c86ace868bc9ba56e",
            "6b5e1376a37042fba1ecd1f2d551e665",
            "03b3fd76b8494006bb282eccd4f27429",
            "2cbb52f6045449d298fde742104a50ee"
          ]
        },
        "id": "evmJyyhZRX1y",
        "outputId": "7e9a247a-ea80-4fc5-995e-6213ee10466c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48249311719b4c42924413c9a9951769"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a589c52118400aa99eb5163ea7a940"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2426eee4fe7464cac7b6efdf2fc956c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0346d1a7710b45bebd120b2ec6c15413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec719bf4e5274f5dbdfb77c8aae06170"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa0d3cc92e4b40b48d13a34eaaed7bad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Tokenize the dataset and align labels\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples['tokens'],\n",
        "        truncation=True,\n",
        "        padding=True,  # Apply padding\n",
        "        max_length=128,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # Only label the first token of a given word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment function to the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Remove columns not needed for training\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"pos_tags\", \"chunk_tags\", \"ner_tags\"])\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "tokenized_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "3209729d092647008344c977a9f55195",
            "967e2c1202b743e3875db359619326ac",
            "47945fdcb9a34499aafa28aa5fc17bc8",
            "0fb8fb8342034e0aa4319c63e1123fd1",
            "7a2aac6621d54bfbb410ca1c76dea53d",
            "bd152b02bd854ec39c18a010bc0cf80e",
            "92920068c48446dca697371cc94f42e6",
            "0d7c7ce3d4d54317b423579e1d0f9f57",
            "a590f857faae42daab9f505d878e5ed8",
            "5965be762f3643c38acb406ced905e97",
            "083f03eaf0494d0ebfac904ec9ebb31e",
            "43e1240c028b429baae5cb63b7d89e9c",
            "c1f40d46384540f28c74754bb3784947",
            "4119155020bf43f396f0bffaa41d020f",
            "5ff74d461f164557babdf5fbc05fc57e",
            "813788f540644351b3284d660330fb84",
            "ff022416882b4ba2a06d313850e7900c",
            "fd5740091c9b4276b26570b3b3cfd693",
            "bacc73cfb1fe45f4b4856c5d4241415d",
            "5b3e6e6f068249bb824a0fbcd72bab5a",
            "f94a3f7d735a4314abf93f3868ec5c18",
            "96c6a47085de4752843653cdfc37acc6",
            "4b347e88d6f743bda9af31931209f677",
            "9f836bc110604ef9a21b849f575ba56b",
            "cea28ec55bdd47c4b3fa4ab52cd7800e",
            "6b0cff397866436bb72d3f71689ea27e",
            "6eba22b64c09442199571b816a4a7f71",
            "1ec3b685f11e4331af53090c6d9d0c07",
            "c8bc424e439a4f888935735d60b7cb9f",
            "2e9846dea3494de99ac363e7e71d4121",
            "a524e15c9ed6460b8eaa015228c483a0",
            "6713ae437c2646a39157f93730a4038f",
            "55734162f1b347b4bb237eec7ce7448e",
            "4a1cc5274fe6420bbb85b19de5adeb44",
            "0e877aa8d99a488fafe0ea1a36e89842",
            "5636caeb443e40228391845844012e29",
            "c8e90b902dec4e2887bd3de5b7189332",
            "84babdc9353844889aa73b7999a91c83",
            "ff4817302a894e07a4691d9338a2d792",
            "84dd19f1c3e14da089df3ead16a86560",
            "e8faf7b4a08f4dc1a44beb1b61c5b5a7",
            "c15cdc6d559d410c988194ac056e352d",
            "e8c34e2ff2ce46108dddd9a10ceb34d7",
            "2008fa22089e4f8981b067279dcee98c",
            "95a6648c083448608d0f303a551ac41f",
            "85a7b44885f349558c4e3b251a68b7d0",
            "595848b86e6d4fa39054b79841d90529",
            "68be18c44c6d46d09465dbd358ec6604",
            "4b9b77d9f1aa4aa184410c165fc6bf41",
            "a989aec63e244e9abc7af11208cf2f91",
            "1259888d132c4e96b9795aad3b04dbd6",
            "8c8f177bb28a411e91435ada7959f200",
            "91a6977562a84882b358bee4d7e3e1dc",
            "c196c8daf6074d32bfbb8bf63bff48f8",
            "59ccd75aa83f44c28b890828f136ee2f",
            "54ccebf6e85a4dda87a6eaf76549e6c6",
            "15947b199cdf4d8fb8838fc9e58aa07e",
            "6194269f0f774c3fa29b5c26eb37cf1e",
            "a7bd74a17f214a7c832ea8e3f215e0b9",
            "93c6f3ea89234c58b70bd5d5943e3fd3",
            "236acf7369d74473ac9edc696234d1b5",
            "7ea216979c7d4bfb940a3ae4e452a2a2",
            "902dbd2505bc4d6dba257dcbb27434e7",
            "46544afa75f245078c41a15eddfeb72e",
            "a43311fa2bd24361b42c31a1f81452e7",
            "9b2ad3062a6646c4987c89a9166638c3",
            "78c1533fbf6c4f078a60d7e25e097f53",
            "cb0274156e7748d191560ae192be81f4",
            "428ea0d3bfdf4ca3afc66966c19fed3e",
            "e852fad5405f44ed84c3b1fc407ccad5",
            "200761162a40472aabe9738d44430dde",
            "25bc420b8e7f49af8d097e2f4346fb50",
            "8edae69b363b477ab039a4060958799a",
            "aae3b785a13f4ddda7bb471081d5436c",
            "e0dd66ffb5784416a9c11b7d90b5266c",
            "8fe0e61d0d274de08c88d8fa6ca3c197",
            "829db9e188234ecd892c3f4e22084243"
          ]
        },
        "id": "5ELVFLlVRcha",
        "outputId": "9334ab23-0501-4f1c-b614-6c9473e15ed2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3209729d092647008344c977a9f55195"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43e1240c028b429baae5cb63b7d89e9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b347e88d6f743bda9af31931209f677"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a1cc5274fe6420bbb85b19de5adeb44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95a6648c083448608d0f303a551ac41f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54ccebf6e85a4dda87a6eaf76549e6c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78c1533fbf6c4f078a60d7e25e097f53"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT model for token classification\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(dataset['train'].features['ner_tags'].feature.names))\n",
        "\n",
        "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "4f3d0e89c53b4f99bb08496dfb29e8ba",
            "ea0e26d3b65e4302b4c4f99a57802b48",
            "e8ad11a2a9b2421d866bb7570c66434d",
            "c0211874d83f404ab205559898e5a2ad",
            "76e6f485da884e9388b2f44b0c649b39",
            "735bffb96e0d451bae98458610e00863",
            "b61b7b48eea74e4aa2b82288ed2842a1",
            "a5b3d1c46d7b49fbb034b64925c60628",
            "23ddc6c9df52454aaf60e4a22f00965d",
            "4b35863acaed49ad8f065536fc193565",
            "2309949c12e143c3908b750b6c39376d"
          ]
        },
        "id": "bQ-pYIdeReKK",
        "outputId": "7bc0d94c-47f5-4161-ade5-0535efba535e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f3d0e89c53b4f99bb08496dfb29e8ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2634/2634 16:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.159100</td>\n",
              "      <td>0.039387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.028300</td>\n",
              "      <td>0.036488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.036638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [204/204 00:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.03663837909698486, 'eval_runtime': 19.3991, 'eval_samples_per_second': 167.534, 'eval_steps_per_second': 10.516, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "g0LrwTrQU1hL",
        "outputId": "4df8dd22-f52d-455a-db40-28642a46f119"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [204/204 01:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.03663837909698486, 'eval_runtime': 19.7711, 'eval_samples_per_second': 164.381, 'eval_steps_per_second': 10.318, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"Hugging Face Inc. is a company based in New York City.\",\n",
        "    \"Bert is a neural network-based technique for natural language processing.\"\n",
        "]\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move inputs to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Move tokenized inputs to the device\n",
        "input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Get the logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Convert logits to predictions\n",
        "predictions = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "\n",
        "# Get the label names\n",
        "label_names = dataset['train'].features['ner_tags'].feature.names\n",
        "\n",
        "# Function to align predictions with the input tokens\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = []\n",
        "    for pred in predictions:\n",
        "        preds.append([label_names[p] for p in pred])\n",
        "    return preds\n",
        "\n",
        "# Align the predictions\n",
        "aligned_predictions = align_predictions(predictions, input_ids.cpu().numpy())\n",
        "\n",
        "# Print the results\n",
        "for sentence, tokens, preds in zip(sentences, tokenized_inputs[\"input_ids\"], aligned_predictions):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    tokenized_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
        "    print(\"Tokenized Tokens:\", tokenized_tokens)\n",
        "    print(\"Predictions:\", preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFf_cystRi0B",
        "outputId": "3d9b77f5-928c-4174-9482-82c62dc38168"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Hugging Face Inc. is a company based in New York City.\n",
            "Tokenized Tokens: ['[CLS]', 'Hu', '##gging', 'Face', 'Inc', '.', 'is', 'a', 'company', 'based', 'in', 'New', 'York', 'City', '.', '[SEP]']\n",
            "Predictions: ['O', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O']\n",
            "Sentence: Bert is a neural network-based technique for natural language processing.\n",
            "Tokenized Tokens: ['[CLS]', 'Bert', 'is', 'a', 'neural', 'network', '-', 'based', 'technique', 'for', 'natural', 'language', 'processing', '.', '[SEP]', '[PAD]']\n",
            "Predictions: ['O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    }
  ]
}